{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ferIn5uvpYGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import re\n",
        "\n",
        "# Download and unzip the dataset\n",
        "url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "response = requests.get(url)\n",
        "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "    text = z.read(z.namelist()[0]).decode('utf-8')\n",
        "\n",
        "# Tokenize the text\n",
        "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n",
        "int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
        "encoded = [vocab_to_int[word] for word in words]\n",
        "\n",
        "# Prepare the dataset\n",
        "seq_length = 50\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded) - seq_length):\n",
        "    sequences.append(encoded[i:i + seq_length])\n",
        "    targets.append(encoded[i + seq_length])\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "dataset = TextDataset(sequences, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self)._init_()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "vocab_size = len(vocab_to_int) + 1  # +1 for padding token\n",
        "embed_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "# Training the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_text, length):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = None, None\n",
        "\n",
        "    for _ in range(length):\n",
        "        x = torch.tensor([[vocab_to_int[w] for w in words[-seq_length:]]], dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            output, (state_h, state_c) = model.lstm(model.embedding(x), (state_h, state_c))\n",
        "            output = model.fc(output[:, -1, :])\n",
        "        word_id = output.argmax(dim=1).item()\n",
        "        words.append(int_to_vocab[word_id])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Generate sample texts\n",
        "start_text = 'anarchism originated as a term of abuse'\n",
        "generated_text = generate_text(model, start_text, 100)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "lwHWQ6nFnPkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}